{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<style type=\"text/css\">\n",
    ".input_prompt, .input_area, .output_prompt {\n",
    "display:none !important;\n",
    "}\n",
    ".reveal h1, .reveal h2 {\n",
    "    font-family:times\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center> Part-of-Speech Taggers and Parsers </center>\n",
    "### <center> CS918 Natural Language Processing </center>\n",
    "\n",
    "###### <center> Bo Wang </center>\n",
    "<h6><center> February 2016 </center></h6>\n",
    "<br/>\n",
    "<center><img src=\"images/786px-Warwick_Crest.png\" width=\"90\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part-of-Speech Tagging\n",
    "\n",
    "* The process of classifying words into their parts of speech (also known as word classes or lexical categories) and labeling them accordingly is known as **part-of-speech tagging or POS-tagging**.\n",
    "* The labels are called **POS tags**. The collection of tags used for a particular task is known as a **tag set** (e.g. Penn Treebank POS tagset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Part-of-Speech Tagger\n",
    "\n",
    "* NLTK\n",
    "* [Stanford](http://nlp.stanford.edu/software/tagger.shtml)\n",
    "* spaCy\n",
    "* [SENNA](http://ml.nec-labs.com/senna/)\n",
    "* [CMU POS tagger for tweets](http://www.cs.cmu.edu/~ark/TweetNLP/index.html)\n",
    "* [Gate twitie-tagger](https://gate.ac.uk/wiki/twitter-postagger.html)\n",
    "<br><br>\n",
    "* More: http://aclweb.org/aclwiki/index.php?title=POS_Tagging_(State_of_the_art)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### NLTK.tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('They', 'PRP'), ('refuse', 'VBP'), ('to', 'TO'), ('permit', 'VB'), ('us', 'PRP'), ('to', 'TO'), ('obtain', 'VB'), ('the', 'DT'), ('refuse', 'NN'), ('permit', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "## NLTK\n",
    "\n",
    "import nltk\n",
    "text = nltk.word_tokenize(\"They refuse to permit us to obtain the refuse permit\")\n",
    "# An off-the-shelf tagger is available. It uses the Penn Treebank tagset by default.\n",
    "print nltk.pos_tag(text) # outputs a list of tuples of the form ('token', 'tag')\n",
    "# 'nltk.help.upenn_tagset()' provides documentation for each tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('fly', 'NN')\n",
      "fly\n",
      "NN\n",
      "\n",
      "\n",
      "[('The', 'AT'), ('grand', 'JJ'), ('jury', 'NN'), ('commented', 'VBD'), ('on', 'IN'), ('a', 'AT'), ('number', 'NN'), ('of', 'IN'), ('other', 'AP'), ('topics', 'NNS'), (',', ','), ('AMONG', 'IN'), ('them', 'PPO'), ('the', 'AT'), ('Atlanta', 'NP'), ('and', 'CC'), ('Fulton', 'NP-TL'), ('County', 'NN-TL'), ('purchasing', 'VBG'), ('departments', 'NNS'), ('which', 'WDT'), ('it', 'PPS'), ('said', 'VBD'), ('``', '``'), ('ARE', 'BER'), ('well', 'QL'), ('operated', 'VBN'), ('and', 'CC'), ('follow', 'VB'), ('generally', 'RB'), ('accepted', 'VBN'), ('practices', 'NNS'), ('which', 'WDT'), ('inure', 'VB'), ('to', 'IN'), ('the', 'AT'), ('best', 'JJT'), ('interest', 'NN'), ('of', 'IN'), ('both', 'ABX'), ('governments', 'NNS'), (\"''\", \"''\"), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# We can create these tuples from the standard string representation \n",
    "# of tagged token using the function str2tuple()\n",
    "tagged_token = nltk.tag.str2tuple('fly/NN')\n",
    "print tagged_token\n",
    "print tagged_token[0]\n",
    "print tagged_token[1]\n",
    "\n",
    "sent = '''\n",
    " The/AT grand/JJ jury/NN commented/VBD on/IN a/AT number/NN of/IN\n",
    " other/AP topics/NNS ,/, AMONG/IN them/PPO the/AT Atlanta/NP and/CC\n",
    " Fulton/NP-tl County/NN-tl purchasing/VBG departments/NNS which/WDT it/PPS\n",
    " said/VBD ``/`` ARE/BER well/QL operated/VBN and/CC follow/VB generally/RB\n",
    " accepted/VBN practices/NNS which/WDT inure/VB to/IN the/AT best/JJT\n",
    " interest/NN of/IN both/ABX governments/NNS ''/'' ./.\n",
    " '''\n",
    "print '\\n'\n",
    "print [nltk.tag.str2tuple(t) for t in sent.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'The', u'DET'), (u'population', u'NOUN'), (u'of', u'ADP'), (u'the', u'DET'), (u'Congo', u'NOUN'), (u'is', u'VERB'), (u'13.5', None), (u'million', None), (u',', None), (u'divided', None), (u'into', None), (u'at', None), (u'least', None), (u'seven', None), (u'major', None), (u'``', None), (u'culture', None), (u'clusters', None), (u\"''\", None), (u'and', None), (u'innumerable', None), (u'tribes', None), (u'speaking', None), (u'400', None), (u'separate', None), (u'dialects', None), (u'.', None)]\n",
      "Accuracy score on test sents = 0.148111232931\n"
     ]
    }
   ],
   "source": [
    "# Training a N-gram tagger\n",
    "\n",
    "# Unigram taggers assign the tag that is most likely for that particular token, based on the training corpus.\n",
    "# An n-gram tagger is a generalization of a unigram tagger whose context is \n",
    "# the current word together with the part-of-speech tags of the n-1 preceding tokens.\n",
    "\n",
    "from nltk.corpus import brown\n",
    "from nltk.tag import UnigramTagger, BigramTagger, DefaultTagger\n",
    "\n",
    "brown_tagged_sents = brown.tagged_sents(categories='news', tagset='universal')\n",
    "brown_sents = brown.sents(categories='news')\n",
    "size = int(len(brown_tagged_sents) * 0.9)\n",
    "train_sents = brown_tagged_sents[:size]\n",
    "test_sents = brown_tagged_sents[size:]\n",
    "bigram_tagger = BigramTagger(train_sents)\n",
    "\n",
    "unseen_sent = brown_sents[4203]\n",
    "print bigram_tagger.tag(unseen_sent)\n",
    "print \"Accuracy score on test sents =\",bigram_tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'The', u'DET'), (u'population', 'NOUN'), (u'of', u'ADP'), (u'the', u'DET'), (u'Congo', 'NOUN'), (u'is', u'VERB'), (u'13.5', 'NOUN'), (u'million', u'NUM'), (u',', u'.'), (u'divided', u'VERB'), (u'into', u'ADP'), (u'at', u'ADP'), (u'least', u'ADJ'), (u'seven', u'NUM'), (u'major', u'ADJ'), (u'``', u'.'), (u'culture', 'NOUN'), (u'clusters', 'NOUN'), (u\"''\", u'.'), (u'and', u'CONJ'), (u'innumerable', 'NOUN'), (u'tribes', 'NOUN'), (u'speaking', u'VERB'), (u'400', u'NUM'), (u'separate', u'ADJ'), (u'dialects', 'NOUN'), (u'.', u'.')]\n",
      "Accuracy score on test sents = 0.920861158178\n"
     ]
    }
   ],
   "source": [
    "# Combining taggers, to address the trade-off between accuracy and coverage\n",
    "default_tagger = DefaultTagger('NOUN')\n",
    "unigram_tagger = UnigramTagger(train_sents, backoff=default_tagger)\n",
    "bigram_tagger = BigramTagger(train_sents, backoff=unigram_tagger)\n",
    "print bigram_tagger.tag(unseen_sent)\n",
    "print \"Accuracy score on test sents =\",bigram_tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully stored tagger!\n",
      "Tagger loaded!\n",
      "[(u'The', u'DET'), (u'population', 'NOUN'), (u'of', u'ADP'), (u'the', u'DET'), (u'Congo', 'NOUN'), (u'is', u'VERB'), (u'13.5', 'NOUN'), (u'million', u'NUM'), (u',', u'.'), (u'divided', u'VERB'), (u'into', u'ADP'), (u'at', u'ADP'), (u'least', u'ADJ'), (u'seven', u'NUM'), (u'major', u'ADJ'), (u'``', u'.'), (u'culture', 'NOUN'), (u'clusters', 'NOUN'), (u\"''\", u'.'), (u'and', u'CONJ'), (u'innumerable', 'NOUN'), (u'tribes', 'NOUN'), (u'speaking', u'VERB'), (u'400', u'NUM'), (u'separate', u'ADJ'), (u'dialects', 'NOUN'), (u'.', u'.')]\n"
     ]
    }
   ],
   "source": [
    "# Storing taggers\n",
    "from pickle import dump\n",
    "output = open('/Users/bowang/Documents/CS918_lab/taggers-parsers-seminar/tagger.pkl', 'wb')\n",
    "dump(bigram_tagger, output)\n",
    "output.close()\n",
    "print \"Successfully stored tagger!\"\n",
    "\n",
    "# Loading taggers\n",
    "from pickle import load\n",
    "input = open('/Users/bowang/Documents/CS918_lab/taggers-parsers-seminar/tagger.pkl', 'rb')\n",
    "tagger = load(input)\n",
    "input.close()\n",
    "print \"Tagger loaded!\"\n",
    "print tagger.tag(unseen_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Stanford Tagger\n",
    "\n",
    "* A Java implementation of the log-linear part-of-speech taggers described in [here](http://nlp.stanford.edu/~manning/papers/emnlp2000.pdf) and [here](http://nlp.stanford.edu/~manning/papers/tagging.pdf).\n",
    "* The full download contains three trained English tagger models, an Arabic tagger model, a Chinese tagger model, a French tagger model, and a German tagger model.\n",
    "* Also has an [English Twitter POS tagger model](https://gate.ac.uk/wiki/twitter-postagger.html) written by Leon Derczynski and others at GATE.\n",
    "* The English taggers use the [Penn Treebank tag set](http://www.cis.upenn.edu/~treebank/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* e.g.\n",
    "\n",
    "\n",
    "```\n",
    "#!/bin/sh\n",
    "#\n",
    "# usage: ./stanford-postagger.sh model textFile\n",
    "\n",
    "java -mx300m -cp 'stanford-postagger.jar:' edu.stanford.nlp.tagger.maxent.MaxentTagger -model models/gate-EN-twitter.model -sentenceDelimiter newline -tokenize false -tagSeparator \\* -textFile $1 > $2\n",
    "```\n",
    "\n",
    "\n",
    "* [Read more..](http://nlp.stanford.edu/software/pos-tagger-faq.shtml)\n",
    "* Also have a look at `README.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'What', u'WP'),\n",
       " (u'is', u'VBZ'),\n",
       " (u'the', u'DT'),\n",
       " (u'airspeed', u'NN'),\n",
       " (u'of', u'IN'),\n",
       " (u'an', u'DT'),\n",
       " (u'unladen', u'JJ'),\n",
       " (u'swallow', u'VB'),\n",
       " (u'?', u'.')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Stanford Tagger\n",
    "# Using Stanford tagger in NLTK\n",
    "\n",
    "#export CLASSPATH=/Users/bowang/Tools/stanford_nlp/stanford-postagger-full-2015-04-20/stanford-postagger.jar\n",
    "#export STANFORD_MODELS=/Users/bowang/Tools/stanford_nlp/stanford-postagger-full-2015-04-20/models\n",
    "\n",
    "import os\n",
    "os.environ.get('CLASSPATH')\n",
    "\n",
    "from nltk.tag import StanfordPOSTagger\n",
    "st = StanfordPOSTagger('english-left3words-distsim.tagger')\n",
    "st.tag('What is the airspeed of an unladen swallow ?'.split()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### POS Tagging using spaCy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(What , u'WP'), (is , u'VBZ'), (the , u'DT'), (airspeed , u'NN'), (of , u'IN'), (an , u'DT'), (unladen , u'JJ'), (swallow , u'NN'), (?, u'.')]\n"
     ]
    }
   ],
   "source": [
    "## spaCy\n",
    "\n",
    "from spacy.en import English\n",
    "\n",
    "tagger = English(parser=False, tagger=True, entity=False)\n",
    "\n",
    "# pos/pos_ : A coarse-grained, less detailed tag that represents the word-class of the token.\n",
    "# tag/tag_ : A fine-grained, more detailed tag that represents the word-class and some basic \n",
    "# morphological information for the token. These tags are primarily designed to be good features\n",
    "# for subsequent models, particularly the syntactic parser. \n",
    "# They are language and treebank dependent.\n",
    "\n",
    "# A property with an underscore at the end returns the string representation.\n",
    "# while a property without the underscore returns an index (int) into spaCy's vocabulary.\n",
    "\n",
    "def print_pos(token):\n",
    "    return (token.tag_)\n",
    "\n",
    "def pos_tags(sentence):\n",
    "    sentence = unicode(sentence, \"utf-8\")\n",
    "    tokens = tagger(sentence)\n",
    "    tags = []\n",
    "    for tok in tokens:\n",
    "        tags.append((tok,print_pos(tok)))\n",
    "    return tags\n",
    "print pos_tags('What is the airspeed of an unladen swallow ?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'There', u'ADV')\n",
      "(u'is', u'VERB')\n",
      "(u'an', u'DET')\n",
      "(u'art', u'NOUN')\n",
      "(u',', u'PUNCT')\n",
      "(u'it', u'NOUN')\n",
      "(u'says', u'VERB')\n",
      "(u',', u'PUNCT')\n",
      "(u'or', u'CONJ')\n",
      "(u'rather', u'ADV')\n",
      "(u',', u'PUNCT')\n",
      "(u'a', u'DET')\n",
      "(u'knack', u'NOUN')\n",
      "(u'to', u'ADP')\n",
      "(u'flying', u'NOUN')\n",
      "(u'.', u'PUNCT')\n"
     ]
    }
   ],
   "source": [
    "multiSentence = \"There is an art, it says, or rather, a knack to flying.\" \\\n",
    "                 \"The knack lies in learning how to throw yourself at the ground and miss.\" \\\n",
    "                 \"In the beginning the Universe was created. This has made a lot of people \"\\\n",
    "                 \"very angry and been widely regarded as a bad move.\"\n",
    "\n",
    "parser2 = English()\n",
    "parsedData = parser2(unicode(multiSentence, \"utf-8\"))\n",
    "sents = []\n",
    "# the \"sents\" property returns spans of each sentence\n",
    "# spans have indices into the original string\n",
    "# where each index value represents a token\n",
    "for span in parsedData.sents:\n",
    "    # go from the start to the end of each span, returning each token in the sentence\n",
    "    sent = [parsedData[i] for i in range(span.start, span.end)]\n",
    "    for token in sent:\n",
    "        print(token.orth_, token.pos_)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'lol', u'NOUN', u'lol')\n",
      "(u'that', u'ADJ', u'that')\n",
      "(u'is', u'VERB', u'be')\n",
      "(u'rly', u'ADV', u'rly')\n",
      "(u'funny', u'ADJ', u'funny')\n",
      "(u':)', u'PUNCT', u':)')\n",
      "(u'This', u'DET', u'this')\n",
      "(u'is', u'VERB', u'be')\n",
      "(u'gr8', u'VERB', u'gr8')\n",
      "(u'i', u'NOUN', u'i')\n",
      "(u'rate', u'VERB', u'rate')\n",
      "(u'it', u'NOUN', u'it')\n",
      "(u'8/8', u'NUM', u'8/8')\n",
      "(u'!', u'PUNCT', u'!')\n",
      "(u'!', u'PUNCT', u'!')\n",
      "(u'!', u'PUNCT', u'!')\n"
     ]
    }
   ],
   "source": [
    "messyData = \"lol that is rly funny :) This is gr8 i rate it 8/8!!!\"\n",
    "parsedData = parser2(unicode(messyData, \"utf-8\"))\n",
    "for token in parsedData:\n",
    "    print(token.orth_, token.pos_, token.lemma_) # a lemma is the canonical form of words..\n",
    "    \n",
    "## It does reasonably well. Though fails on the token \"gr8\" and \"lol\"\n",
    "## \"gr8\" should be an adjective rather than a verb\n",
    "## and \"lol\" is more like an interjection than a noun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### CMU POS tagger for tweets\n",
    "\n",
    "* Java-based tokenizer and part-of-speech tagger for tweets.\n",
    "* Manually labeled POS annotated tweets as training data.\n",
    "* https://github.com/brendano/ark-tweet-nlp/\n",
    "* Its POS tagset can be found in its [original paper](http://www.cs.cmu.edu/~ark/TweetNLP/gimpel+etal.acl11.pdf).\n",
    "     * Including some Twitter/online-specific tags, such as '#' for hashtag, '@' for at-mention and 'E' for emoticon.\n",
    "     \n",
    "* A python wrapper can be found [here](https://github.com/ianozsvald/ark-tweet-nlp-python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Options:\n",
    "  --model <Filename>        Specify model filename. (Else use built-in.)\n",
    "  --just-tokenize           Only run the tokenizer; no POS tags.\n",
    "  --quiet                   Quiet: no output\n",
    "  --input-format <Format>   Default: auto\n",
    "                            Options: json, text, conll\n",
    "  --output-format <Format>  Default: automatically decide from input format.\n",
    "                            Options: pretsv, conll\n",
    "  --input-field NUM         Default: 1\n",
    "                            Which tab-separated field contains the input\n",
    "                            (1-indexed, like unix 'cut')\n",
    "                            Only for {json, text} input formats.\n",
    "  --word-clusters <File>    Alternate word clusters file (see FeatureExtractor)\n",
    "  --no-confidence           Don't output confidence probabilities\n",
    "  --decoder <Decoder>       Change the decoding algorithm (default: greedy)\n",
    "\n",
    "e.g. $ ./runTagger.sh --no-confidence --output-format conll examples/example_tweets.txt > output.txt\n",
    "\n",
    "To see more info use $ ./runTagger.sh --help\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Parsing\n",
    "\n",
    "* A natural language parser is a program (i.e. a formal and computational method) that works out the grammatical structure of sentences.\n",
    "    * For instances, which groups of words go together (as \"phrases\") and which words are the subject or object of a verb.\n",
    "* Resulting in a parse tree showing the syntactic relation to each part of the sentence, which may also contain semantic and other information.\n",
    "    * Thus the patterns of well-formedness and ill-formedness in a sequence of words can be understood with respect to the phrase structure and dependencies.\n",
    "* A key motivation is natural language understanding. For examples:\n",
    "    * How much more of the meaning of a text can we access when we can reliably recognize the linguistic structures it contains? \n",
    "    * Can a program \"understand\" it enough to be able to answer simple questions about \"what happened\" or \"who did what to whom\"?\n",
    "<br><br>\n",
    "* Has a wide range of applications.\n",
    "    * For example, we would expect the natural language questions submitted to a question-answering system to undergo parsing as an initial step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Dependency parsing\n",
    "\n",
    "* Parse trees are usually constructed based on either the constituency relation of constituency grammars (phrase structure grammars) or the dependency relation of dependency grammars.\n",
    "    * Phrase structure grammar is concerned with how words and sequences of words combine to form constituents.\n",
    "    * Dependency grammar focusses instead on how words relate to other words.\n",
    "* Dependency is a binary asymmetric relation that holds between a **head** and its **dependents**.\n",
    "    * The head of a sentence is usually taken to be the tensed verb, and every other word is either dependent on the sentence head, or connects to it through a path of dependencies.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Dependency parsing\n",
    "\n",
    "\n",
    "\n",
    "<center><img src=\"images/depgraph0.png\" width=\"600\"></center>\n",
    "\n",
    "* The arcs in the image above are labeled with the grammatical function that holds between a dependent and its head. For example, I is the `SBJ` (subject) of *shot* (which is the head of the whole sentence), and in is an `NMOD` (noun modifier of *elephant*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Dependency Parser\n",
    "\n",
    "* [Stanford Parser](http://nlp.stanford.edu/software/lex-parser.shtml)\n",
    "* spaCy\n",
    "* [CMU TweeboParser](http://www.cs.cmu.edu/~ark/TweetNLP/#tweeboparser_tweebank)\n",
    "* [MaltParser](http://www.maltparser.org/)\n",
    "<br><br>\n",
    "* http://aclweb.org/aclwiki/index.php?title=Parsing_(State_of_the_art)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Stanford Parser\n",
    "\n",
    "* A Java implementation of probabilistic natural language parsers.\n",
    "* Has a GUI for viewing the phrase structure tree output of the parser.\n",
    "* The parser provides [Universal Dependencies and Stanford Dependencies](http://nlp.stanford.edu/software/stanford-dependencies.shtml) output as well as phrase structure trees.\n",
    "* Can be used in NLTK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* e.g.\n",
    "\n",
    "\n",
    "```\n",
    "java -mx2g -cp \"*\" edu.stanford.nlp.parser.lexparser.LexicalizedParser -sentences newline -tokenized -tagSeparator § -tokenizerFactory edu.stanford.nlp.process.WhitespaceTokenizer -tokenizerMethod newCoreLabelTokenizerFactory -escaper edu.stanford.nlp.process.PTBEscapingProcessor -outputFormat typedDependencies edu/stanford/nlp/models/lexparser/englishPCFG.caseless.ser.gz sample-input.tagged.txt > sample-input.conll2007.txt\n",
    "```\n",
    "\n",
    "\n",
    "* Use `-sentences` as your sentence delimitator. For example, ff you want to give the parser one sentence per line, include the option `-sentences newline` in your command line.\n",
    "* If your input text has already been tokenised, you can give the option `-tokenized` then the parser will assume white-space separated tokens and use your tokenization as is.\n",
    "    *  A common occurrence is that your text is already correctly tokenized but does not escape characters the way the Penn Treebank does. In this case you can also add the flag `-escaper edu.stanford.nlp.process.PTBEscapingProcessor`.\n",
    "* You can also give the parser already POS-tagged input, by adding relevant options `-sentences`, `-tokenized`, `-tokenizerFactory`, `-tokenizerMethod`, and `-tagSeparator`.\n",
    "    * Make sure your input is correctly tokenised and uses the correct tag names.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* `-outputFormat` selects the style of the output. \n",
    "    * *penn, oneline, rootSymbolOnly, words, wordsAndTags, dependencies, typedDependencies, typedDependenciesCollapsed, latexTree, xmlTree, collocations, semanticGraph, conllStyleDependencies, conll2007*\n",
    "<br><br>\n",
    "* You can read more about Stanford Parser and its available options/flags at [here](http://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/parser/lexparser/LexicalizedParser.html) and [here](http://nlp.stanford.edu/software/parser-faq.shtml)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ROOT\n",
      "  (S\n",
      "    (NP (DT this))\n",
      "    (VP (VBZ is) (NP (DT the) (JJ english) (NN parser) (NN test)))))\n",
      "(ROOT\n",
      "  (S\n",
      "    (NP (DT the) (NN parser))\n",
      "    (VP (VBZ is) (PP (IN from) (NP (JJ stanford) (NN parser))))))\n"
     ]
    }
   ],
   "source": [
    "## Stanford Parser\n",
    "# Using Stanford parser in NLTK\n",
    "import os\n",
    "from nltk.parse.stanford import StanfordParser\n",
    "\n",
    "parser_path = '/Users/bowang/Tools/stanford_nlp/stanford-parser-full-2015-04-20/stanford-parser.jar'\n",
    "model_path = '/Users/bowang/Tools/stanford_nlp/stanford-parser-full-2015-04-20/stanford-parser-3.5.2-models.jar'\n",
    "os.environ['STANFORD_PARSER'] = parser_path\n",
    "os.environ['STANFORD_MODELS'] = model_path\n",
    "\n",
    "parser = StanfordParser(model_path=\"edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz\")\n",
    "parsed = parser.raw_parse_sents((\"this is the english parser test\", \"the parser is from stanford parser\"))\n",
    "for myListiterator in parsed:\n",
    "    for t in myListiterator:\n",
    "        print t\n",
    "\n",
    "# _OUTPUT_FORMAT = 'penn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[((u'jumps', u'VBZ'), u'nsubj', (u'fox', u'NN')), ((u'fox', u'NN'), u'det', (u'The', u'DT')), ((u'fox', u'NN'), u'amod', (u'quick', u'JJ')), ((u'fox', u'NN'), u'amod', (u'brown', u'JJ')), ((u'jumps', u'VBZ'), u'nmod', (u'dog', u'NN')), ((u'dog', u'NN'), u'case', (u'over', u'IN')), ((u'dog', u'NN'), u'det', (u'the', u'DT')), ((u'dog', u'NN'), u'amod', (u'lazy', u'JJ'))]]\n",
      "\n",
      "\n",
      "[Tree('jumps', [Tree('fox', ['The', 'quick', 'brown']), Tree('dog', ['over', 'the', 'lazy'])])]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "\n",
    "dep_parser=StanfordDependencyParser(model_path=\"edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz\")\n",
    "sent = \"The quick brown fox jumps over the lazy dog.\"\n",
    "print [list(parse.triples()) for parse in dep_parser.raw_parse(sent)]\n",
    "print '\\n'\n",
    "print [parse.tree() for parse in dep_parser.raw_parse(sent)]\n",
    "\n",
    "# _OUTPUT_FORMAT = 'conll2007'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Parsing using spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the airspeed of an unladen swallow ?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(u'What', u'acomp', u'is', [], []),\n",
       " (u'is', u'dep', u'an', [u'What'], [u'the', u'airspeed']),\n",
       " (u'the', u'det', u'is', [], []),\n",
       " (u'airspeed', u'nmod', u'is', [], [u'of']),\n",
       " (u'of', u'prep', u'airspeed', [], []),\n",
       " (u'an', u'meta', u'unladen', [u'is'], []),\n",
       " (u'unladen', u'ROOT', u'unladen', [u'an'], [u'swallow', u'?']),\n",
       " (u'swallow', u'dep', u'unladen', [], []),\n",
       " (u'?', u'nmod', u'unladen', [], [])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.en import English\n",
    "\n",
    "parser = English(parser=True, tagger=False, entity=False)\n",
    "def print_pos(token):\n",
    "    # returns (original token, dependency tag, head word, left dependents, right dependents)\n",
    "    return (token.orth_, token.dep_, token.head.orth_, [t.orth_ for t in token.lefts], [t.orth_ for t in token.rights])\n",
    "\n",
    "def pos_tags(sentence):\n",
    "    sentence = unicode(sentence, \"utf-8\")\n",
    "    tokens = parser(sentence)\n",
    "    tags = []\n",
    "    for tok in tokens:\n",
    "        tags.append(print_pos(tok))\n",
    "    return tags\n",
    "sent = 'What is the airspeed of an unladen swallow ?'\n",
    "print sent\n",
    "pos_tags(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### CMU TweeboParser\n",
    "\n",
    "* A dependency parser for English tweets, developed by [Kong et al. (2014)](http://www.cs.cmu.edu/~nasmith/papers/kong+schneider+swayamdipta+bhatia+dyer+smith.emnlp14.pdf).\n",
    "* Trained on a subset of a new labeled corpus for 929 tweets (12,318 tokens) drawn from the POS-tagged tweet corpus of [Owoputi et al. (2013)](http://www.cs.cmu.edu/~ark/TweetNLP/owoputi+etal.naacl13.pdf), Tweebank.\n",
    "* Pro:\n",
    "    * Has some interesting and useful features (especially for parsing tweets), such as support for multi-word annotations and multiple roots.\n",
    "* Con:\n",
    "    * Instead of the popular Penn Treebank annotation it uses a simpler annotation scheme and outputs much less dependency type information.\n",
    "* https://github.com/ikekonglp/TweeboParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "e.g.\n",
    "\n",
    "<center><img src=\"images/tweeboparser1.png\" width=\"800\"></center>\n",
    "\n",
    "\n",
    "\n",
    "<center><img src=\"images/tweeboparser2.png\" width=\"400\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Resources:\n",
    "\n",
    "* [A Good Part-of-Speech Tagger in about 200 Lines of Python](https://spacy.io/blog/part-of-speech-POS-tagger-in-python)\n",
    "* [Stanford Typed Dependencies Manual](http://nlp.stanford.edu/software/dependencies_manual.pdf)\n",
    "* [Stanford CoreNLP](http://stanfordnlp.github.io/CoreNLP/)\n",
    "* The NLTK book can be found [here](http://www.nltk.org/book/)\n",
    "* spaCy Doc: https://spacy.io/docs\n",
    "* Source code for [nltk.tag.stanford](http://www.nltk.org/_modules/nltk/tag/stanford.html) and [nltk.parse.stanford](http://www.nltk.org/_modules/nltk/parse/stanford.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thank you!\n"
     ]
    }
   ],
   "source": [
    "print \"Thank you!\""
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
